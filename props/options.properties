h:Print this help\n

# IO
i:Name/Path of an input (ARFF or CSV) file p containing feature vectors over time\n\
  The first feature must be a string or number which specifies all feature vectors which belong to one analysis window/instance.
attributes:An optional string, specifying all input attributes (mandatory in case of multiple labels): \n\
           n=name, t=time stamp, 0=text feature, 1-9=numeric feature, c=class label/numeric label, r=remove attribute\n\
           Using different numbers for numeric features will create a separate codebook and bag for all features belonging to the same index.\n\
           The codebook index is followed by brackets [] specifying the number of consecutive input features belonging to this index.\n\
           Example: -attributes nt1[14]2[14]c\n\
           Input file with the structure: name,timestamp,28 numeric features split into two codebooks (14 features each) and one label.
o:Name / Path of an output ARFF, CSV or LibSVM file p containing the bag-of-features representation.\n\
  The file format depends on the given file ending (*.arff, *.csv or *.libsvm).
oi:Name / Path of an output CSV file p containing the word indexes.
writeName:Output the id string/number in the output file (only ARFF & CSV).
writeTimeStamp:Output the time stamp in the output file (only ARFF & CSV).
l:CSV file p with the class labels for each analysis window/instance.\n\
  Both nominal and numeric classes are supported. Format:\n\
  1st line: name (according to the input file); label1; label2; ...\n\
  2nd line: 'file_1.wav'; class1; ...\n\
            [and so on]\n

# Preprocessing options
t:Segment the input files with a windows size (segment width) of p1 seconds and a hop size (shift) of p2 seconds\n\
  If this option is used, the second column of the input file must be a time index (in seconds) of the current frame and \n\
  the (optional) labels file must have three columns (name; time; label).\n
e:Remove all feature vectors from the input, where the activity (or energy) is below p2. Index p1 specifies the index of the activity attribute (first index: 1).
standardizeInput:Standardize all numeric input features.\n\
  The parameters are stored in the codebook file (-B) and then used for standardization of test data (-b) in an online approach.
normalizeInput:Normalize all numeric input features (min->max is normalized to 0->1).\n\
  The parameters are stored in the codebook file (-B) and then used for standardization of test data (-b) in an online approach.

# Codebook (only numeric features)
size:Set the (initial) size p of the codebook. (default: size=500)
c:Method of creating the codebook:\n\
  p=random (default): Generate the codebook by random sampling of the input feature vectors.\n\
  p=random++: Generate the codebook by random sampling of the input feature vectors with weighting similiar to initialization of kmeans++.\n\
  p=kmeans: Employ kmeans clustering (Lloyd's algorithm).\n\
  p=kmeans++: Employ kmeans++ clustering (Lloyd's algorithm).
reduce:Reduce the size of the codebook by merging words which are correlated with each other. PCC with threshold p is considered.
supervised:Generate a codebook for each class separately, first, then merge all codebooks. (Not available for numeric labels.)
seed:Select the random seed p used for codebook creation. (Has no effect on training selection configured by -numTrain).
numTrain:Randomly choose p feature vectors from the input data for the creation of the codebook (should not be used for random sampling).
svq:If >= 1, split vector quantization (SVQ) is used. The feature vectors are split into p1 sub-vectors. Codebooks are created for each sub-vector.\n\
  Then, a final codebook and bag is created from the assigned indexes of the sub-vectors. p2 specifies the size of the sub-codebooks.\n

b:Load codebook p (do not create one).
B:Save the created codebook as a file p.\n

# Assignment (only numeric)
a:When creating the bag-of-features, assign each feature vector p vectors from the quantized vectors in the codebook. (default: a=1)
gaussian:Soft assignment using Gaussian encoding with standard deviation p.\n

# Assignment (only text)
minTermFreq:Gives a minimum threshold for the number of occurrences of each word/n-gram to be considered for codebook generation (default: minTermFreq=1)
maxTermFreq:Gives a maximum threshold for the number of occurrences of each word/n-gram to be considered for codebook generation (default: minTermFreq=0(inf))
stopChar:Specifies characters which are removed from all input instances (default: .,;:()?!* )
nGram:N-gram (default: nGram=1)
nCharGram:N-character-gram (default: nCharGram=0)\n

# Assignment general
log:Logarithmic term weighting 'lg(TF+1)' of the term frequency.\n\
    This parameter is stored in the codebook file (-B) and used when the respective codebook is loaded (-b).
idf:Inverse document frequency transform: Multiply the term frequency (TF) with the logarithm of the ratio of the \n\
    total number of instances and the number of instances where the respective word is present.\n\
    This parameter is stored in the codebook file (-B) and used when the respective codebook is loaded (-b).
norm:Normalize the bag-of-features (3 options of normalization).\n\
     p=1: Divides the term frequencies (TF) by the number of input frames.\n\
     p=2: Divides the TF by the sum of all TFs.\n\
     p=3: Divides the TF by a factor so that the resulting Euclidean length is 1.\n

# Postprocessing options
standardizeOutput:Standardize all output features (term frequencies).\n\
  The parameters are stored in the codebook file (-B) and then used for standardization of test data (-b) in an online approach.
normalizeOutput:Normalize all output features (term frequencies, min->max is normalized to 0->1).\n\
  The parameters are stored in the codebook file (-B) and then used for standardization of test data (-b) in an online approach.
